{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sickopickle/Coursera_Deep_Learning_Specialization/blob/main/Project_(fixed_fraction_bar_and_removed_complex_functions_and_spatial_relations).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1ews68Rxwok",
        "outputId": "095e90cf-b2e8-45b0-fff1-1ffd608de960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!sud                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              `````````````````````````````````````````                                                                     ,,,o add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!sudo apt-get update -qq 2>&1 > /dev/null\n",
        "!sudo apt -y install -qq google-drive-ocamlfuse 2>&1 > /dev/null\n",
        "!google-drive-ocamlfuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "aO1H2KLXaaZs"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LQRJa8fxxfD",
        "outputId": "9c21ba29-f835-495a-887f-0b5bb1fb4fe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "mkdir: cannot create directory ‘drive’: File exists\n",
            "/content/drive\n",
            "mkdir: cannot create directory ‘MyDrive’: File exists\n",
            "/content\n",
            "/\n",
            "fuse: mountpoint is not empty\n",
            "fuse: if you are sure this is safe, use the 'nonempty' mount option\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -qq w3m # to act as web browser \n",
        "!xdg-settings set default-web-browser w3m.desktop # to set default browser\n",
        "%cd /content\n",
        "!mkdir drive\n",
        "%cd drive\n",
        "!mkdir MyDrive\n",
        "%cd ..\n",
        "%cd ..\n",
        "!google-drive-ocamlfuse /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ZRPX-ja5Liwt"
      },
      "outputs": [],
      "source": [
        "data_file_path=\"C:/Users/aiden/Downloads/ProjectData/LGINKML/\"\n",
        "lg_file_path=data_file_path+\"LGs/\"\n",
        "inkml_file_path=data_file_path+\"INKMLs/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "eEFqm8abrABO"
      },
      "outputs": [],
      "source": [
        "def getSoup(file):\n",
        "  with open(file, \"r\") as infile:\n",
        "    xml_string = infile.read()\n",
        "  soup = BeautifulSoup(xml_string, \"xml\")\n",
        "  return soup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "WRVVtA8QWsbo"
      },
      "outputs": [],
      "source": [
        "def arrayToPoints (arr) :\n",
        "  points = []\n",
        "  for i in arr:\n",
        "    points.append(point(float(i[0]), float(i[1])))\n",
        "  return(points)\n",
        "  \n",
        "def PointsToArray (points) :\n",
        "  arr=[]\n",
        "  for point in points:\n",
        "    arr.append([point.x,point.y])\n",
        "  return arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "SucWG0X3Liwy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "EPSILON = 1e-12\n",
        "MACHINE_EPSILON = 1.12e-16\n",
        "\n",
        "\n",
        "class point:\n",
        "  def __init__(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "def isMachineZero (val) : \n",
        "  return (val >= -MACHINE_EPSILON and val <= MACHINE_EPSILON)\n",
        "def hypot (x, y) : \n",
        "  return math.sqrt(x * x + y * y)\n",
        "def pointLength (p) : \n",
        "  return hypot(p.x, p.y)\n",
        "def pointNegate (p) : \n",
        "  return point(-p.x, -p.y)\n",
        "def pointAdd (p1, p2) : \n",
        "  return point(p1.x + p2.x, p1.y + p2.y)\n",
        "def pointSubtract (p1, p2) : \n",
        "  return point(p1.x - p2.x, p1.y - p2.y)\n",
        "def pointMultiplyScalar (p, n) : \n",
        "  return point(p.x * n, p.y * n)\n",
        "def pointDot (p1, p2) : \n",
        "  return p1.x * p2.x + p1.y * p2.y\n",
        "def pointDistance (p1, p2) : \n",
        "  return hypot(p1.x - p2.x, p1.y - p2.y)\n",
        "def pointNormalize (p, length = 1) : \n",
        "  return pointMultiplyScalar(p, (length / pointLength(p)) if pointLength(p) else 0) #POSSIBLE FLAW\n",
        "class createSegment: \n",
        "  def __init__(self, p, i, o):\n",
        "    self.p=p\n",
        "    self.i=i\n",
        "    self.o=o #POSSIBLE FLAW (CHANGED)\n",
        "\n",
        "def chordLengthParameterize (points, first, last)  :\n",
        "    u = [0]\n",
        "    for i in range(first+1, last+1):\n",
        "      u.append(u[i - first - 1] + pointDistance(points[i], points[i - 1]))\n",
        "    \n",
        "    for i in range(1, last-first+1):\n",
        "    #for (let i = 1, m = last - first i <= m i++) :\n",
        "        u[i] /= u[last-first]\n",
        "    \n",
        "    return u\n",
        "\n",
        "# Use Newton-Raphson iteration to find better root.\n",
        "def findRoot (curve, point, u) :\n",
        "    curve1 = []\n",
        "    curve2 = []\n",
        "    # Generate control vertices for Q'\n",
        "    for i in range(3) : #FLAG\n",
        "        curve1.append(pointMultiplyScalar(pointSubtract(curve[i + 1], curve[i]), 3))\n",
        "    \n",
        "    # Generate control vertices for Q''\n",
        "    for i in range(2):\n",
        "    #for (let i = 0 i <= 1 i++) :\n",
        "        curve2.append(pointMultiplyScalar(pointSubtract(curve1[i + 1], curve1[i]), 2))\n",
        "    \n",
        "    # Compute Q(u), Q'(u) and Q''(u)\n",
        "    pt = evaluate(3, curve, u)\n",
        "    pt1 = evaluate(2, curve1, u)\n",
        "    pt2 = evaluate(1, curve2, u)\n",
        "    diff = pointSubtract(pt, point)\n",
        "    df = pointDot(pt1, pt1) + pointDot(diff, pt2)\n",
        "    # u = u - f(u) / f'(u)\n",
        "    #return isMachineZero(df) ? u : u - pointDot(diff, pt1) / df\n",
        "    return (u if isMachineZero(df) else (u-pointDot(diff,pt1)/df))\n",
        "\n",
        "# Evaluate a bezier curve at a particular parameter value\n",
        "def evaluate (degree, curve, t) :\n",
        "    # Copy array\n",
        "    tmp = curve[:]#.slice() FLAG\n",
        "    # Triangle computation\n",
        "    for i in range(1, degree+1):\n",
        "    #for (let i = 1 i <= degree i++) :\n",
        "        for j in range(degree):\n",
        "        #for (let j = 0 j <= degree - i j++) :\n",
        "            tmp[j] = pointAdd(pointMultiplyScalar(tmp[j], 1 - t), pointMultiplyScalar(tmp[j + 1], t))\n",
        "\n",
        "    return tmp[0]\n",
        "\n",
        "def addCurve (segments, curve)  :\n",
        "    prev = segments[len(segments) - 1]\n",
        "    prev.o = pointSubtract(curve[1], curve[0])\n",
        "    segments.append(createSegment(curve[3], pointSubtract(curve[2], curve[3]), None))\n",
        "\n",
        "# Use least-squares method to find Bezier control points for region.\n",
        "def generateBezier (points, first, last, uPrime, tan1, tan2)  :\n",
        "    epsilon = EPSILON\n",
        "    #abs = Math.abs\n",
        "    pt1 = points[first]\n",
        "    pt2 = points[last]\n",
        "    # Create the C and X matrices\n",
        "    C = [[0, 0],[0, 0]]\n",
        "    X = [0, 0]\n",
        "    for i in range(0, last-first+1):\n",
        "    #for (let i = 0, l = last - first + 1 i < l i++) :\n",
        "        u = uPrime[i] \n",
        "        t = 1 - u\n",
        "        b = 3 * u * t\n",
        "        b0 = t * t * t\n",
        "        b1 = b * t\n",
        "        b2 = b * u\n",
        "        b3 = u * u * u\n",
        "        a1 = pointNormalize(tan1, b1)\n",
        "        a2 = pointNormalize(tan2, b2)\n",
        "        tmp = pointSubtract(pointSubtract(points[first + i], pointMultiplyScalar(pt1, b0 + b1)), pointMultiplyScalar(pt2, b2 + b3))\n",
        "        C[0][0] += pointDot(a1, a1)\n",
        "        C[0][1] += pointDot(a1, a2)\n",
        "        # C[1][0] += a1.dot(a2)\n",
        "        C[1][0] = C[0][1]\n",
        "        C[1][1] += pointDot(a2, a2)\n",
        "        X[0] += pointDot(a1, tmp)\n",
        "        X[1] += pointDot(a2, tmp)\n",
        "    \n",
        "    # Compute the determinants of C and X\n",
        "    detC0C1 = C[0][0] * C[1][1] - C[1][0] * C[0][1]\n",
        "    alpha1=None\n",
        "    alpha2=None\n",
        "    if (abs(detC0C1) > epsilon) :\n",
        "        # Kramer's rule\n",
        "        detC0X = C[0][0] * X[1] - C[1][0] * X[0]\n",
        "        detXC1 = X[0] * C[1][1] - X[1] * C[0][1]\n",
        "        # Derive alpha values\n",
        "        alpha1 = detXC1 / detC0C1\n",
        "        alpha2 = detC0X / detC0C1\n",
        "    \n",
        "    else :\n",
        "        # Matrix is under-determined, try assuming alpha1 == alpha2\n",
        "        c0 = C[0][0] + C[0][1]\n",
        "        c1 = C[1][0] + C[1][1]\n",
        "        #alpha1 = alpha2 = abs(c0) > epsilon ? X[0] / c0 : abs(c1) > epsilon ? X[1] / c1 : 0\n",
        "        alpha1 = alpha2 = X[0] / c0 if abs(c0) > epsilon else X[1] / c1 if abs(c1) > epsilon else 0\n",
        "    \n",
        "    # If alpha negative, use the Wu/Barsky heuristic (see text)\n",
        "    # (if alpha is 0, you get coincident control points that lead to\n",
        "    # divide by zero in any subsequent NewtonRaphsonRootFind() call.\n",
        "    segLength = pointDistance(pt2, pt1)\n",
        "    eps = epsilon * segLength\n",
        "    handle1 = None\n",
        "    handle2 = None\n",
        "    if (alpha1 < eps or alpha2 < eps) :\n",
        "        # fall back on standard (probably inaccurate) formula,\n",
        "        # and subdivide further if needed.\n",
        "        alpha1 = alpha2 = segLength / 3\n",
        "    \n",
        "    else :\n",
        "        # Check if the found control points are in the right order when\n",
        "        # projected onto the line through pt1 and pt2.\n",
        "        line = pointSubtract(pt2, pt1)\n",
        "        # Control points 1 and 2 are positioned an alpha distance out\n",
        "        # on the tangent vectors, left and right, respectively\n",
        "        handle1 = pointNormalize(tan1, alpha1)\n",
        "        handle2 = pointNormalize(tan2, alpha2)\n",
        "        if (pointDot(handle1, line) - pointDot(handle2, line) > segLength * segLength) :\n",
        "            # Fall back to the Wu/Barsky heuristic above.\n",
        "            alpha1 = alpha2 = segLength / 3\n",
        "            handle1 = handle2 = None # Force recalculation\n",
        "        \n",
        "    \n",
        "    # First and last control points of the Bezier curve are\n",
        "    # positioned exactly at the first and last data points\n",
        "    return [pt1, pointAdd(pt1, handle1 or pointNormalize(tan1, alpha1)), pointAdd(pt2, handle2 or pointNormalize(tan2, alpha2)), pt2]\n",
        "\n",
        "# Given set of points and their parameterization, try to find\n",
        "# a better parameterization.\n",
        "def reparameterize (points, first, last, u, curve) :\n",
        "    #for (let i = first i <= last i++) :\n",
        "    for i in range(first, last+1):\n",
        "        u[i - first] = findRoot(curve, points[i], u[i - first])\n",
        "    \n",
        "    # Detect if the new parameterization has reordered the points.\n",
        "    # In that case, we would fit the points of the path in the wrong order.\n",
        "    #for (let i = 1, l = u.length i < l i++) :\n",
        "    for i in range (1, len(u)):\n",
        "        if (u[i] <= u[i - 1]):\n",
        "            return False\n",
        "    \n",
        "    return True\n",
        "\n",
        "\n",
        "class error:\n",
        "  def __init__(self, error, index):\n",
        "    self.error = error\n",
        "    self.index = index\n",
        "# Find the maximum squared distance of digitized points to fitted curve.\n",
        "def findMaxError (points, first, last, curve, u)  :\n",
        "    index = math.floor((last - first + 1) / 2)\n",
        "    maxDist = 0\n",
        "    #for (let i = first + 1 i < last i++) :\n",
        "    for i in range (first+1, last):\n",
        "        P = evaluate(3, curve, u[i - first])\n",
        "        v = pointSubtract(P, points[i])\n",
        "        dist = v.x * v.x + v.y * v.y # squared\n",
        "        if (dist >= maxDist) :\n",
        "            maxDist = dist\n",
        "            index = i\n",
        "        \n",
        "    return error(maxDist, index)\n",
        "\n",
        "def fit (points, closed, error) :\n",
        "    length = len(points)\n",
        "    if length == 0 and type(length) == int:\n",
        "        return []\n",
        "    segments = [createSegment(points[0], None, None)]\n",
        "    if length == 1:\n",
        "      return segments\n",
        "    fitCubic(points, segments, error, 0, length - 1, \n",
        "    pointSubtract(points[1], points[0]), \n",
        "    pointSubtract(points[length - 2], points[length - 1]))\n",
        "    if (closed) :\n",
        "        segments.pop(0)\n",
        "        segments.pop()\n",
        "    \n",
        "    return segments\n",
        "\n",
        "def fitCubic (points, segments, error, first, last, tan1, tan2):\n",
        "    #  Use heuristic if region only has two points in it\n",
        "    if (last - first == 1) :\n",
        "        pt1 = points[first]\n",
        "        pt2 = points[last]\n",
        "        dist = pointDistance(pt1, pt2) / 3\n",
        "        addCurve(segments, [pt1, pointAdd(pt1, pointNormalize(tan1, dist)), pointAdd(pt2, pointNormalize(tan2, dist)), pt2])\n",
        "        return\n",
        "    \n",
        "    # Parameterize points, and attempt to fit curve\n",
        "    uPrime = chordLengthParameterize(points, first, last)\n",
        "    maxError = max(error, error * error)\n",
        "    split=None\n",
        "    parametersInOrder = True\n",
        "    # Try not 4 but 5 iterations\n",
        "    #for (let i = 0 i <= 4 i++) :\n",
        "    for i in range(5):\n",
        "        curve = generateBezier(points, first, last, uPrime, tan1, tan2)\n",
        "        #  Find max deviation of points to fitted curve\n",
        "        Max = findMaxError(points, first, last, curve, uPrime)\n",
        "        if (Max.error < error and parametersInOrder) :\n",
        "            addCurve(segments, curve)\n",
        "            return\n",
        "        \n",
        "        split = Max.index\n",
        "        # If error not too large, try reparameterization and iteration\n",
        "        if (Max.error >= maxError):\n",
        "            break\n",
        "        parametersInOrder = reparameterize(points, first, last, uPrime, curve)\n",
        "        maxError = Max.error\n",
        "    \n",
        "    # Fitting failed -- split at max error point and fit recursively\n",
        "    tanCenter = pointSubtract(points[split - 1], points[split + 1])\n",
        "    fitCubic(points, segments, error, first, split, tan1, tanCenter)\n",
        "    fitCubic(points, segments, error, split, last, pointNegate(tanCenter), tan2)\n",
        "\n",
        "\n",
        "def filterRepeats_Normalize(points, start_x, min_y, max_y, precision_b=250, first_offstroke_dist=30):\n",
        "  filtered_points=[]\n",
        "  for i in range(len(points)):\n",
        "    if i==len(points)-1 or points[i][0] != points[i+1][0] or points[i][1] != points[i+1][1]:\n",
        "      y_range=(max_y-min_y if max_y-min_y != 0 else 1)\n",
        "      filtered_points.append((np.array(points[i])-np.array([start_x-first_offstroke_dist*y_range/precision_b, min_y]))*precision_b/y_range)\n",
        "      #filtered_points.append((np.array(points[i])-np.array([min_x, min_y]))*precision_b/(max_y-min_y if max_y-min_y != 0 else 1))\n",
        "  return filtered_points\n",
        "\n",
        "\n",
        "# Assign parameter values to digitized points\n",
        "# using relative distances between points.\n",
        "\n",
        "def svgPath (segments, closed, precision) :\n",
        "    length = len(segments)\n",
        "    precisionMultiplier = 10 ** precision\n",
        "    def Round(n, precisionMultiplier):\n",
        "      return ((round(n * precisionMultiplier) / precisionMultiplier) if precision < 16 else n)\n",
        "    def formatPair (x, y):\n",
        "      return (str(Round(x, precisionMultiplier)) + ',' + str(Round(y, precisionMultiplier)))\n",
        "    first = True\n",
        "    prevX = None\n",
        "    prevY = None\n",
        "    outX = None\n",
        "    outY = None\n",
        "    parts = []\n",
        "    if length==1:\n",
        "      return 'M' + str(segments[0].p.x) + \",\" + str(segments[0].p.y) \n",
        "    \n",
        "    if (not length):\n",
        "        return ''\n",
        "    for i in range(length):\n",
        "        segment=segments[i]\n",
        "        #repeat=False\n",
        "        curX = segment.p.x\n",
        "        curY = segment.p.y\n",
        "        if (first) :\n",
        "            parts.append('M' + formatPair(curX, curY))\n",
        "        else :\n",
        "            inX = curX + (segment.i.x if segment.i else 0)\n",
        "            inY = curY + (segment.i.y if segment.i else 0)\n",
        "            if (not (inX == curX and inY == curY and outX == prevX and outY == prevY)) :\n",
        "                parts.append('c' +\n",
        "                      formatPair(outX - prevX, outY - prevY) +\n",
        "                      ' ' +\n",
        "                      formatPair(inX - prevX, inY - prevY) +\n",
        "                      ' ' +\n",
        "                      formatPair(curX - prevX, curY - prevY))\n",
        "\n",
        "        prevX = curX\n",
        "        prevY = curY\n",
        "        outX = curX + (segment.o.x if segment.o else 0)\n",
        "        outY = curY + (segment.o.y if segment.o else 0)\n",
        "        first = False\n",
        "\n",
        "    return ''.join(parts)\n",
        "\n",
        "def getSegmentsPathData (segments, closed, precision)  :\n",
        "    length = len(segments)\n",
        "    precisionMultiplier = 10 ** precision\n",
        "    def Round(n, precisionMultiplier):\n",
        "      return ((round(n * precisionMultiplier) / precisionMultiplier) if precision < 16 else n)\n",
        "    def formatPair (x, y):\n",
        "      return (str(Round(x, precisionMultiplier)) + ',' + str(Round(y, precisionMultiplier)))\n",
        "    first = True\n",
        "    prevX = None\n",
        "    prevY = None\n",
        "    outX = None\n",
        "    outY = None\n",
        "    parts = []\n",
        "    if length==1:\n",
        "      return ([[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1]],(segments[0].p.x,segments[0].p.y),(segments[0].p.x,segments[0].p.y))\n",
        "    \n",
        "    if (not length):\n",
        "        return []\n",
        "    allSegmentData=[]\n",
        "    for i in range(length):\n",
        "      segmentData = []\n",
        "      segment=segments[i]\n",
        "      curX = segment.p.x\n",
        "      curY = segment.p.y\n",
        "      if (first) :\n",
        "        startPoints=(curX,curY)\n",
        "      else :\n",
        "          inX = curX + (segment.i.x if segment.i else 0)\n",
        "          inY = curY + (segment.i.y if segment.i else 0)\n",
        "          if (not (inX == curX and inY == curY and outX == prevX and outY == prevY)) :\n",
        "                # c = relative curveto:\n",
        "              segmentData.extend([prevX, prevY, outX, outY, inX, inY, curX, curY])\n",
        "              parts.append('c' +\n",
        "                    formatPair(outX - prevX, outY - prevY) +\n",
        "                    ' ' +\n",
        "                    formatPair(inX - prevX, inY - prevY) +\n",
        "                    ' ' +\n",
        "                    formatPair(curX - prevX, curY - prevY))\n",
        "              features = [segmentData[6]-segmentData[0], \n",
        "                          segmentData[7]-segmentData[1], \n",
        "                          math.sqrt((segmentData[0]-segmentData[2])**2+(segmentData[1]-segmentData[3])**2), \n",
        "                          math.sqrt((segmentData[6]-segmentData[4])**2+(segmentData[7]-segmentData[5])**2), \n",
        "                          math.atan2(segmentData[3]-segmentData[1], segmentData[2]-segmentData[0])-math.atan2(segmentData[7]-segmentData[1], segmentData[6]-segmentData[0]),\n",
        "                          math.atan2(segmentData[5]-segmentData[7], segmentData[4]-segmentData[6])-math.atan2(segmentData[1]-segmentData[7], segmentData[0]-segmentData[6]), \n",
        "                          1]\n",
        "              allSegmentData.append(features)\n",
        "              endPoints=(segmentData[6],segmentData[7])\n",
        "\n",
        "      prevX = curX\n",
        "      prevY = curY\n",
        "      outX = curX + (segment.o.x if segment.o else 0)\n",
        "      outY = curY + (segment.o.y if segment.o else 0)\n",
        "      first = False\n",
        "    return (allSegmentData, startPoints, endPoints)\n",
        "\n",
        "class options:\n",
        "  def __init__(self, closed, tolerance, precision):\n",
        "    self.closed = closed\n",
        "    self.tolerance = tolerance\n",
        "    self.precision=precision\n",
        "\n",
        "def f1(p):\n",
        "  return point(p.x, p.y)\n",
        "\n",
        "def f2(p):\n",
        "  return point(float(p[0]), float(p[1]))\n",
        "\n",
        "def getSvgPath (points, options) :\n",
        "    if (len(points) == 0 and type(len(points))==int) :\n",
        "        return ''\n",
        "    return svgPath(fit(list(map(f2, points)), options.closed, options.tolerance if options.tolerance else 2.5), options.closed, options.precision if options.precision else 5)\n",
        "\n",
        "def getFeatures (points, options)  :\n",
        "    if (len(points) == 0 and type(len(points))==int) :\n",
        "        return []\n",
        "    return getSegmentsPathData(fit(list(map(f2, points)), options.closed, options.tolerance if options.tolerance else 2.5), options.closed, options.precision if options.precision else 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "UHcFenQELiw6"
      },
      "outputs": [],
      "source": [
        "error_files=[]\n",
        "bezier_curve_errors=[]\n",
        "parse_traces_errors=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "GR2AhqjZBKzA"
      },
      "outputs": [],
      "source": [
        "from timeit import default_timer\n",
        "def _parse_one_trace(trace_string):\n",
        "  trace_split=trace_string.string\n",
        "  if trace_split[0]==\"\\n\":\n",
        "    trace_split=trace_split[1:]\n",
        "  if trace_split[-1]==\"\\n\":\n",
        "    trace_split=trace_split[:-1]\n",
        "  trace_split=trace_split.split(\",\")\n",
        "  a = [[float(s) for s in entry.strip().split(\" \")[:2]] for entry in trace_split]\n",
        "  return a\n",
        "\n",
        "\n",
        "def getSvg(file, tolerance=15, precision_a=5, precision_b=250, first_offstroke_dist=30):\n",
        "  #_, order = convert_relations(convert_inkml_to_lg(file))\n",
        "  with open(file, \"r\") as infile:\n",
        "    xml_string = infile.read()\n",
        "  soup = BeautifulSoup(xml_string, \"xml\")\n",
        "  trace_strings = soup.find_all(\"trace\")\n",
        "  a=\"\"\n",
        "  list_points=[]\n",
        "  #order_dict=reorder(file.replace(\"INKMLs\", \"LGs\").replace(\".inkml\", \".lg\"))\n",
        "  #for i in order:\n",
        "  #  trace_string=trace_strings[i]\n",
        "  #  list_points.append(_parse_one_trace(trace_string))\n",
        "  for trace_string in trace_strings:\n",
        "    list_points.append(_parse_one_trace(trace_string))\n",
        "  array_points=tf.ragged.constant(list_points)\n",
        "  mins=tf.math.reduce_min(array_points, axis=(0,1))\n",
        "  maxs=tf.math.reduce_max(array_points, axis=(0,1))\n",
        "  #min_x=float(mins[0])\n",
        "  min_y=float(mins[1])\n",
        "  max_y=float(maxs[1])\n",
        "  for points in list_points:\n",
        "    points=filterRepeats_Normalize(points,list_points[0][0][0],min_y,max_y,precision_b, first_offstroke_dist)\n",
        "    path=getSvgPath(points, options(False,tolerance,precision_a))\n",
        "    a+=path\n",
        "  return a \n",
        "\n",
        "def parse_traces(file, order, tolerance=15, precision_a=5, precision_b=250, first_offstroke_dist=100):\n",
        "  with open(file, \"r\") as infile:\n",
        "    xml_string = infile.read()\n",
        "  soup = BeautifulSoup(xml_string, \"xml\")\n",
        "  trace_strings = soup.find_all(\"trace\")\n",
        "  a=[]\n",
        "  first = True\n",
        "  prevEnd = None\n",
        "  list_points=[]\n",
        "  for i in order:\n",
        "    try:\n",
        "      trace_string=trace_strings[i]\n",
        "    except:\n",
        "      parse_traces_errors.append(trace_string)\n",
        "    list_points.append(_parse_one_trace(trace_string))\n",
        "  mins=tf.math.reduce_min(tf.ragged.constant(list_points), axis=(0,1))\n",
        "  maxs=tf.math.reduce_max(tf.ragged.constant(list_points), axis=(0,1))\n",
        "  min_y=float(mins[1])\n",
        "  max_y=float(maxs[1])\n",
        "  for points in list_points:\n",
        "    points=filterRepeats_Normalize(points,list_points[0][0][0],min_y,max_y,precision_b, first_offstroke_dist)\n",
        "    features=getFeatures(points, options(False,tolerance,precision_a))\n",
        "    if first:\n",
        "      a += [[first_offstroke_dist, precision_b/2-features[1][1], 0.0, 0.0, 0.0, 0.0, 0]]\n",
        "      #NOTE if using the math.sqrt(dx**2....) then modify the above line as well\n",
        "    else:\n",
        "      try:\n",
        "        dx=features[1][0]-prevEnd[0]\n",
        "        dy=features[1][1]-prevEnd[1]\n",
        "        #dx=features[1][len(features[1])-1][0] - prev[1][len(prev[1])-1][0]\n",
        "        #dy=features[1][len(features[1])-1][1]-prev[1][len(prev[1])-1][1]\n",
        "        #NOTE change indices 2 and 3 to 0 experimentally.\n",
        "        #NOTE added extra on-strokes so the model can output blanks\n",
        "        #NOTE changed from on-strokes to off-strokes\n",
        "        a += [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0]]\n",
        "        a += [[dx, dy, 0.0, 0.0, 0.0, 0.0, 0]]\n",
        "        a += [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0]]\n",
        "        #a += [[dx, dy, math.sqrt(dx**2/9+dy**2/9), math.sqrt(dx**2/9+dy**2/9), 0.0, 0.0, 0]]\n",
        "      except:\n",
        "        error_files.append(file)\n",
        "      \n",
        "    a += features[0]\n",
        "    prevEnd = features[2]\n",
        "    first=False\n",
        "  a += [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0]]\n",
        "  a += [[first_offstroke_dist, precision_b/2-prevEnd[1], 0.0, 0.0, 0.0, 0.0, 0]]\n",
        "  a += [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0]]\n",
        "  return a "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "DgjCCwG_Liw9"
      },
      "outputs": [],
      "source": [
        "def convert_inkml_to_lg(file):\n",
        "  return(\"FeatureData/LGs/\"+file.split(\"/\")[2]+\"/\"+file.split(\"/\")[3][:-5]+\"lg\")\n",
        "  #return (\"/\".join(file.split(\"/\")[:6])+\"/LGs/\"+\"/\".join(file.split(\"/\")[7:])[:-5]+\"lg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "hSpLriEUG64-"
      },
      "outputs": [],
      "source": [
        "#old y data generation\n",
        "\n",
        "from collections import defaultdict\n",
        "# todo: Add something to return the sequence of unique identifiers ('x_1' etc.)\n",
        "def parse_symbols_and_alignments(file):\n",
        "  with open(file, \"r\") as f:\n",
        "    string = f.read()\n",
        "  #lg=[]\n",
        "  rel=0 #BOOLEAN sorta\n",
        "  objects=[]\n",
        "  rels=defaultdict(lambda: \"NoRel\")\n",
        "  for part in string.split(\"\\n\"):\n",
        "    line=part.split(\", \")\n",
        "    if (line[0]==\"# Relations from SRT:\"):\n",
        "      rel=1\n",
        "    else:\n",
        "      if (line[0]!=''):\n",
        "        if rel:\n",
        "          #if rel > 1:\n",
        "          rels[(line[1], line[2])]=line[3]\n",
        "          #rel+=1\n",
        "          #prev=line[1]\n",
        "        else:\n",
        "          if (line[0][0]==\"#\"):\n",
        "            continue\n",
        "          del line[3]\n",
        "          objects.append((line[1:3], [int(l) for l in line[3:]]))\n",
        "  def key(obj):\n",
        "    return min(obj[1])\n",
        "  objects=sorted(objects, key=key)\n",
        "  output=[]\n",
        "\n",
        "  output.append(objects[0][0][1])\n",
        "  prev=objects[0][0][0]\n",
        "  #print(rels)\n",
        "  for ob in objects[1:]:\n",
        "    #print((ob[0][0], prev))\n",
        "    output.append(rels[(prev, ob[0][0])])\n",
        "    output.append(ob[0][1])\n",
        "    prev=ob[0][0]\n",
        "    #output.append(ob[0][1])\n",
        "    #output.append(rels[ob[0][0]])\n",
        "  '''try:\n",
        "    output.append(objects[-1][0][1])\n",
        "  except:\n",
        "    print(file)'''\n",
        "  #return(objects, rels)\n",
        "  return(output)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "SBd71hS4Liw_"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "  def __init__(self, symbol, relations, parent, strokes):\n",
        "    #self.id = id\n",
        "    self.symbol = symbol\n",
        "    self.relations = relations\n",
        "    self.parent = parent\n",
        "    self.strokes = strokes\n",
        "  '''def get_root(self, nodes):\n",
        "    node=self\n",
        "    while(nodes[node].parent!=None):\n",
        "      node=nodes[node].parent\n",
        "    #node=Node(self.symbol,self.relations,self.parent,self.strokes)\n",
        "    #while(nodes[node.parent].parent != None):\n",
        "    #  node=nodes[node.parent]\n",
        "    return node'''\n",
        "\n",
        "def get_root(node, nodes):\n",
        "    while(nodes[node].parent!=None):\n",
        "      node=nodes[node].parent\n",
        "    return node\n",
        "\n",
        "\n",
        "#nodes={\"A_1\" : Node(\"A_1\", \"A\", [\"Right\"], [\"x_1\"], None, ), }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "qP2y1SCiLixA"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "failures=0\n",
        "def lg_to_tree(file):\n",
        "  with open(file, \"r\") as f:\n",
        "    string = f.read()\n",
        "  rel=0 #BOOLEAN sorta\n",
        "  nodes=defaultdict(lambda: Node(None, [], None, []))\n",
        "  nodes_inside_relation=defaultdict(list)\n",
        "  for part in string.split(\"\\n\"):\n",
        "    line=part.split(\", \")\n",
        "    if (line[0]==\"# Relations from SRT:\"):\n",
        "      rel=1\n",
        "    else:\n",
        "      if (line[0]!=''):\n",
        "        if rel:\n",
        "          #if rel > 1:\n",
        "          if line[3] == \"Inside\":\n",
        "            nodes_inside_relation[line[1]].append(line[2])\n",
        "          nodes[line[1]].relations.append([line[2],line[3]])\n",
        "          nodes[line[2]].parent=line[1]\n",
        "          #rels[(line[1], line[2])]=line[3]\n",
        "          #rel+=1\n",
        "          #prev=line[1]\n",
        "        else:\n",
        "          if (line[0][0]==\"#\"):\n",
        "            continue\n",
        "          del line[3]\n",
        "          nodes[line[1]].symbol=line[2]\n",
        "          nodes[line[1]].strokes=[int(stroke) for stroke in line[3:]]\n",
        "  for node in nodes_inside_relation:\n",
        "    targets=nodes_inside_relation[node]\n",
        "    if len(targets)>2:\n",
        "      raise ValueError(\"more than 2 inside relation detected\")\n",
        "    if len(targets)==2:\n",
        "      if targets[1] in [r[0] for r in nodes[targets[0]].relations]:\n",
        "        for i, rel in enumerate(nodes[node].relations):\n",
        "          if rel[0]==targets[1]:\n",
        "            target_index=i\n",
        "            break\n",
        "        del nodes[node].relations[target_index]\n",
        "      elif targets[0] in [r[0] for r in nodes[targets[1]].relations]:\n",
        "        for i, rel in enumerate(nodes[node].relations):\n",
        "          if rel[0]==targets[0]:\n",
        "            target_index=i\n",
        "            break\n",
        "        del nodes[node].relations[target_index]\n",
        "      else:\n",
        "        print(\"-----FAILURE------\")\n",
        "        failures+=1\n",
        "        return None\n",
        "\n",
        "  \n",
        "  return(nodes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "u-7rqJ15LixB"
      },
      "outputs": [],
      "source": [
        "#length=0\n",
        "\n",
        "\n",
        "def traverse_tree(tree, current, order, length):\n",
        "  strokes=current.strokes\n",
        "  relations_order={\"Above\":1, \"Below\":2, \"Inside\":3, \"Sup\":4, \"Sub\":5, \"Right\":6}\n",
        "  relations=sorted(current.relations, key = lambda rel: relations_order[rel[1]])\n",
        "  def _add_current_strokes(strokes,length):\n",
        "    strokes=sorted(strokes)\n",
        "    for x in range(len(strokes)):\n",
        "      order[strokes[x]]=length\n",
        "      length+=1\n",
        "    return length\n",
        "  def _traverse_children(tree,relations,order,length):\n",
        "    for i in range(len(relations)):\n",
        "      order_length=traverse_tree(tree, tree[relations[i][0]], order, length)\n",
        "      order=order_length[0]\n",
        "      length=order_length[1]\n",
        "    return length\n",
        "  if len(relations)>0:\n",
        "    if relations[0][1]==\"Above\":\n",
        "      length = _traverse_children(tree,[relations[0]],order,length)\n",
        "      length = _add_current_strokes(strokes,length)\n",
        "      length = _traverse_children(tree,[relations[1]],order,length)\n",
        "      #print(\"ABOVE\",current.symbol, length)\n",
        "      #length = _add_current_strokes(strokes,length)\n",
        "    else:\n",
        "      length = _add_current_strokes(strokes,length)\n",
        "      #print(\"OTHER\",current.symbol,length)\n",
        "      length = _traverse_children(tree,relations,order,length)\n",
        "  else:\n",
        "    length = _add_current_strokes(strokes,length)\n",
        "  #global length\n",
        "  \n",
        "  \n",
        "  #if len(relations)==0:\n",
        "  #print(current.symbol, length)\n",
        "  return((order, length))\n",
        "\n",
        "def reorder(file):\n",
        "  tree=lg_to_tree(file)\n",
        "  order = []\n",
        "  root = get_root(list(tree.keys())[0], tree)\n",
        "  #for i in range(len(root.strokes)):\n",
        "    #order[root.strokes[i]]=i\n",
        "  #global length\n",
        "  length=0\n",
        "  return(traverse_tree(tree, tree[root], order, length)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "1YJc2TrgLixB"
      },
      "outputs": [],
      "source": [
        "def convert_sequence(sequence):\n",
        "    symbols=sequence[::2]\n",
        "    relations=sequence[1::2]\n",
        "    return(symbols, relations)\n",
        "#NOTe MAKE UTF NOT APPEAR FOR SUMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ukjtXD5sLixC",
        "outputId": "1db0b224-2858-42dd-8eb5-477ad4a6ddc7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\\\frac{1}{r^{2}}=\\\\frac{1}{(R-m)^{2}}+\\\\frac{1}{(R+m)^{2}}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "#\\frac{9x^2 + 5}{2}\n",
        "\n",
        "#symbols = ['9','x','2','-','2','fractionbar','5','\\\\cdot ','x','2','fractionbar','5']\n",
        "#relations = [\"Right\", \"Sup\", \"DFS\", \"UTF\", \"NTB\", \"BTD\", \"UFD\", \"Right\", \"Sub\", \"OST\", \"BTD\"]\n",
        "\n",
        "symbols, relations=convert_sequence(['1', 'NTB', '-', 'BTD', 'r', 'Sup', '2', 'DFS-UFD', '=', 'UTF', '1', 'NTB', '-', 'BTD', '(', 'Right', 'R', 'Right', '-', 'Right', 'm', 'Right', ')', 'Sup', '2', 'DFS-UFD', '+', 'UTF', '1', 'NTB', '-', 'BTD', '(', 'Right', 'R', 'Right', '+', 'Right', 'm', 'Right', ')', 'Sup', '2'])\n",
        "\n",
        "def parse_latex(symbols, relations):\n",
        "  utf=0\n",
        "  open_brackets=0\n",
        "  latex_string=symbols[0]\n",
        "  for i in range(len(relations)):\n",
        "    s=symbols[i+1]\n",
        "    full_rel=relations[i]\n",
        "    split_rel=full_rel.split('-')\n",
        "    add_symbol=True\n",
        "    for rel in split_rel:\n",
        "      if rel in [\"DFS\", \"UFS\", \"OFI\", \"UFD\", \"UFL\", \"DFL\"]:\n",
        "        latex_string+=\"}\"\n",
        "        open_brackets-=1\n",
        "      if rel == \"NTB\":\n",
        "        add_symbol=False\n",
        "        if utf:\n",
        "          latex_string+=(\"}{\")\n",
        "          utf-=1\n",
        "        else:\n",
        "          latex_string=\"\\\\frac{\"+latex_string+\"}{\"\n",
        "          open_brackets+=1\n",
        "      if rel == \"UTF\":\n",
        "        latex_string+=(\"\\\\frac{\"+s)\n",
        "        open_brackets+=1\n",
        "        utf+=1\n",
        "        add_symbol=False\n",
        "      if rel in [\"Sub\", \"LB\", \"DTI\"]:\n",
        "        latex_string+=(\"_{\")\n",
        "        open_brackets+=1\n",
        "      if rel == \"Sup\":\n",
        "        latex_string+=(\"^{\")\n",
        "        open_brackets+=1\n",
        "      if rel == \"Inside\":\n",
        "        latex_string+=(\"{\")\n",
        "        open_brackets+=1\n",
        "      if rel == \"STS\":\n",
        "        latex_string+=(\"}_{\")\n",
        "      if rel == \"ITL\":\n",
        "        latex_string+=(\"}^{\")\n",
        "      if rel == \"Radical\":\n",
        "        latex_string+=\"[\"\n",
        "      if rel == \"RTI\":\n",
        "        latex_string+=\"]{\"\n",
        "        open_brackets+=1\n",
        "    if add_symbol:\n",
        "      latex_string+=s\n",
        "  latex_string+=(\"}\")*open_brackets\n",
        "  return latex_string\n",
        "  \n",
        "parse_latex(symbols, relations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "RvDHMIiOLixE"
      },
      "outputs": [],
      "source": [
        "missing_relations=set()\n",
        "missing_relations_freq=defaultdict(lambda: 0)\n",
        "pop_errors=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Wq9pVMPsLixF"
      },
      "outputs": [],
      "source": [
        "arr=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t1bXNIfxUxa",
        "outputId": "88b2445e-d1a5-4de0-d40d-6efb8ef67464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "XBaxs6K_LixG"
      },
      "outputs": [],
      "source": [
        "import pickle as pkl\n",
        "from collections import defaultdict\n",
        "with open(\"FeatureData/Vocab.pkl\", \"rb\") as f:\n",
        "    vocab = pkl.load(f)\n",
        "last_symbol=None\n",
        "new_relations=[]\n",
        "new_rels=defaultdict(lambda: [])\n",
        "def traverse_relations(tree, current, new_rels, incoming_rel, prev_symbol, order):\n",
        "  if tree[current].symbol==\"\\sum\":\n",
        "    relations_order={\"Below\":1, \"Above\":2, \"Inside\":3, \"Sup\":4, \"Sub\":5, \"Right\":6}\n",
        "  else:\n",
        "    relations_order={\"Above\":1, \"Below\":2, \"Inside\":3, \"Sup\":4, \"Sub\":5, \"Right\":6}\n",
        "  \n",
        "  #print(current)\n",
        "  relations=sorted(tree[current].relations, key = lambda rel: relations_order[rel[1]])\n",
        "  last_symbol=current\n",
        "  if len(relations) > 0:\n",
        "    no_rel=False\n",
        "    #tree[current].symbol == \"-\" and \n",
        "    if relations[0][1]==\"Above\":\n",
        "      if tree[current].symbol == \"\\int\":\n",
        "        relations[0][1]=\"Sup\"\n",
        "        if relations[1][1]==\"Below\":\n",
        "          relations[1][1]=\"Sub\"\n",
        "        new_rels[current].append(\"Sup\")\n",
        "      elif tree[current].symbol == \"\\sum\":\n",
        "        new_rels[current].append(\"STL\")\n",
        "      elif tree[current].symbol == \"\\sqrt\":\n",
        "        new_rels[current].append(\"Radical\")\n",
        "      elif prev_symbol and len(new_rels[prev_symbol]):\n",
        "        #if tree[prev_symbol].relations\n",
        "        no_rel=True\n",
        "        if new_rels[prev_symbol][-1]==\"Right\":\n",
        "          new_rels[prev_symbol].pop()\n",
        "        #print(\"prev\",prev_symbol)\n",
        "        new_rels[prev_symbol].append(\"UTF\")\n",
        "      else:\n",
        "        no_rel=True\n",
        "    elif relations[0][1]==\"Below\":\n",
        "      if tree[current].symbol == \"\\sum\":\n",
        "        new_rels[current].append(\"DTI\")\n",
        "      else:\n",
        "        new_rels[current].append(\"LB\")\n",
        "    else:\n",
        "      new_rels[current].append(relations[0][1])\n",
        "    if not no_rel and len(new_rels[current])==1:\n",
        "      strokes=sorted(tree[current].strokes)\n",
        "      #print(strokes)\n",
        "      for x in strokes:\n",
        "        order.append(x)\n",
        "    for i in range(len(relations)):\n",
        "      thing=traverse_relations(tree, relations[i][0], new_rels, relations[i][1], last_symbol, order)#CHANGED\n",
        "      last_symbol=thing[0]\n",
        "      new_rels=thing[1]\n",
        "      #length=thing[2]\n",
        "      order=thing[2]\n",
        "      if relations[i][1]==\"Above\":\n",
        "        if tree[current].symbol==\"-\":\n",
        "          tree[current].symbol=\"frac\"\n",
        "          new_rels[current].append(\"BTD\")\n",
        "          if len(new_rels[current])==1:\n",
        "            strokes=sorted(tree[current].strokes)\n",
        "            for x in strokes:\n",
        "              order.append(x)\n",
        "              #length+=1\n",
        "  else:\n",
        "    new_rels[current]=[]\n",
        "  add=True\n",
        "  if incoming_rel==\"Above\":\n",
        "    if tree[prev_symbol].symbol==\"-\" or tree[prev_symbol].symbol==\"frac\": #CHANGE BECAUSE PREV SYMBOL IS THE LEAF OF SUM NOT SUM ITSELF\n",
        "      #do stuff with sum\n",
        "      new_rels[last_symbol].append(\"NTB\")\n",
        "    elif tree[prev_symbol].symbol==\"\\sqrt\":\n",
        "      new_rels[last_symbol].append(\"RTI\")\n",
        "    else:\n",
        "      new_rels[last_symbol].append(\"DFL\")\n",
        "  elif incoming_rel==\"Below\":\n",
        "    if tree[prev_symbol].symbol==\"\\lim\":\n",
        "      new_rels[last_symbol].append(\"UFL\")\n",
        "    elif tree[prev_symbol].symbol==\"\\sum\":\n",
        "      new_rels[last_symbol].append(\"ITL\")\n",
        "    else:\n",
        "      new_rels[last_symbol].append(\"UFD\")\n",
        "  elif incoming_rel==\"Sub\":\n",
        "    new_rels[last_symbol].append(\"UFS\")\n",
        "  elif incoming_rel==\"Sup\":\n",
        "    if tree[prev_symbol].symbol==\"\\int\":\n",
        "      new_rels[last_symbol].append(\"STS\")\n",
        "    else:\n",
        "      new_rels[last_symbol].append(\"DFS\")\n",
        "  elif incoming_rel==\"Inside\":\n",
        "    new_rels[last_symbol].append(\"OFI\")\n",
        "  else:\n",
        "    add=False\n",
        "  if add:\n",
        "    if len(new_rels[last_symbol])==1:\n",
        "      strokes=sorted(tree[last_symbol].strokes)\n",
        "      for x in strokes:\n",
        "        order.append(x)\n",
        "  return (last_symbol, new_rels, order)\n",
        "\n",
        "def convert_relations(file):\n",
        "  tree=lg_to_tree(file)\n",
        "  root = get_root(list(tree.keys())[0], tree)\n",
        "  order=[]\n",
        "  new_rels=defaultdict(lambda: [])\n",
        "  new_rels[\"S\"]=[\"Right\"]\n",
        "  try:\n",
        "    traverse_output=traverse_relations(tree, root, new_rels, \"Right\", \"S\", order)\n",
        "    new_rels=traverse_output[1]\n",
        "    order=traverse_output[2]\n",
        "    output=[]\n",
        "    not_in_vocab=False\n",
        "    for symbol in new_rels:\n",
        "      if symbol != \"S\":\n",
        "        output.append(vocab[tree[symbol].symbol])\n",
        "      if len(new_rels[symbol]):\n",
        "        relation=\"-\".join(new_rels[symbol])\n",
        "        if relation in vocab:\n",
        "          output.append(vocab[relation])\n",
        "        else:\n",
        "          missing_relations.add(relation)\n",
        "          missing_relations_freq[relation]+=1\n",
        "          not_in_vocab=True\n",
        "          relation=relation.split(\"-\")\n",
        "          if relation.count(\"Right\")>1 or relation.count(\"Inside\")>1 or relation.count(\"Sup\")>1 or relation.count(\"Sub\")>1 or relation.count(\"BTD\")>1:\n",
        "            return (0, None) #0 is used to indentify this specific error from this function's output when generating the data\n",
        "  except:\n",
        "    print(\"pop error: \"+file)\n",
        "    pop_errors.append(file)\n",
        "    return (None, None)\n",
        "\n",
        "  if not_in_vocab:\n",
        "    return (None, None)\n",
        "  if len(output)%2==0:\n",
        "    output.append(0)\n",
        "  #if len(output)%2==0:\n",
        "  #  output.pop()\n",
        "  return (output, order)\n",
        "\n",
        "def get_latex(file):\n",
        "  arr=[]\n",
        "  tree=lg_to_tree(file)\n",
        "  root = get_root(list(tree.keys())[0], tree)\n",
        "  order=[]\n",
        "  new_rels=defaultdict(lambda: [])\n",
        "  new_rels=traverse_relations(tree, root, new_rels, None, None, order)[1]\n",
        "  not_in_vocab=False\n",
        "  for symbol in new_rels:\n",
        "    if tree[symbol].symbol==\"COMMA\":\n",
        "      arr.append(\",\")\n",
        "    else:\n",
        "      arr.append(tree[symbol].symbol)\n",
        "    if len(new_rels[symbol]):\n",
        "      relation=\"-\".join(new_rels[symbol])\n",
        "      if relation==\"DFS-Sub\":\n",
        "        relation=\"STS\"\n",
        "      if relation in vocab:\n",
        "        arr.append(relation)\n",
        "      else:\n",
        "        print(\"missing: \" +relation)\n",
        "        missing_relations.add(relation)\n",
        "        missing_relations_freq[relation]+=1\n",
        "        not_in_vocab=True\n",
        "  \n",
        "  if not_in_vocab:\n",
        "    return (None, None)\n",
        "  \n",
        "  if len(arr)%2==0:\n",
        "    arr.pop()\n",
        "  converted_sequence=convert_sequence(arr)\n",
        "  return parse_latex(converted_sequence[0], converted_sequence[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "wZP_q-nfLixH"
      },
      "outputs": [],
      "source": [
        "def add_to_vocab(symbol, vocab, index):\n",
        "    for key in vocab:\n",
        "        if vocab[key]>=index:\n",
        "            vocab[key]+=1\n",
        "    vocab[symbol]=index\n",
        "    vocab=dict(sorted(vocab.items(), key=lambda item: item[1]))\n",
        "    #with open(\"C:/Users/aiden/Downloads/ProjectData/Vocab.pkl\", \"wb\") as f:\n",
        "    #    pkl.dump(vocab, f)\n",
        "def remove_from_vocab(vocab, index, symbol=None):\n",
        "    for key in vocab:\n",
        "        if vocab[key]>=index:\n",
        "            vocab[key]-=1\n",
        "    if symbol:\n",
        "        vocab[symbol]=index\n",
        "    vocab=dict(sorted(vocab.items(), key=lambda item: item[1]))\n",
        "    #with open(\"C:/Users/aiden/Downloads/ProjectData/Vocab.pkl\", \"wb\") as f:\n",
        "    #    pkl.dump(vocab, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "FLnZv_8aLixH"
      },
      "outputs": [],
      "source": [
        "#add_to_vocab(\"frac\", vocab, 48)\n",
        "#vocab=dict(sorted(vocab.items(), key=lambda item: item[1]))\n",
        "#with open(\"FeatureData/Vocab.pkl\", \"wb\") as f:\n",
        "#    pkl.dump(vocab, f)\n",
        "#with open(\"FeatureData/index_to_symbol.pkl\", \"wb\") as f:\n",
        "#    pkl.dump(index_to_symbol, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpyHKnXsJXov",
        "outputId": "a1055135-e6df-47e3-9007-3c0ace287e45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "missing file:\n",
            "FeatureData/LGs/2014train/formulaire021-equation023 (8bf4d083).lg\n",
            "formulaire021-equation023 (8bf4d083).inkml\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "missing file:\n",
            "FeatureData/LGs/2014train/formulaire019-equation025 (c9228215).lg\n",
            "formulaire019-equation025 (c9228215).inkml\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "3500\n",
            "3600\n",
            "3700\n",
            "3800\n",
            "3900\n",
            "4000\n",
            "4100\n",
            "4200\n",
            "4300\n",
            "4400\n",
            "4500\n",
            "4600\n",
            "4700\n",
            "4800\n",
            "4900\n",
            "5000\n",
            "missing file:\n",
            "FeatureData/LGs/2014train/formulaire019-equation029 (1acd9714).lg\n",
            "formulaire019-equation029 (1acd9714).inkml\n",
            "5100\n",
            "5200\n",
            "5300\n",
            "5400\n",
            "missing file:\n",
            "FeatureData/LGs/2014train/formulaire019-equation030 (6987c839).lg\n",
            "formulaire019-equation030 (6987c839).inkml\n",
            "5500\n",
            "5600\n",
            "missing file:\n",
            "FeatureData/LGs/2014train/formulaire019-equation028 (87b51f17).lg\n",
            "formulaire019-equation028 (87b51f17).inkml\n",
            "5700\n",
            "5800\n",
            "5900\n",
            "6000\n",
            "6100\n",
            "missing file:\n",
            "FeatureData/LGs/2014train/formulaire019-equation026 (7c37a40d).lg\n",
            "formulaire019-equation026 (7c37a40d).inkml\n",
            "6200\n",
            "6300\n",
            "6400\n",
            "6500\n",
            "6600\n",
            "6700\n",
            "6800\n",
            "missing file:\n",
            "FeatureData/LGs/2014train/formulaire021-equation019 (cd75546c).lg\n",
            "formulaire021-equation019 (cd75546c).inkml\n",
            "6900\n",
            "7000\n",
            "7100\n",
            "7200\n",
            "7300\n",
            "7400\n",
            "7500\n",
            "7600\n",
            "missing file:\n",
            "FeatureData/LGs/2014train/formulaire019-equation033 (9ef68406).lg\n",
            "formulaire019-equation033 (9ef68406).inkml\n",
            "7700\n",
            "7800\n",
            "7900\n",
            "8000\n",
            "8100\n",
            "8200\n",
            "8300\n",
            "8400\n",
            "8500\n",
            "8600\n"
          ]
        }
      ],
      "source": [
        "import pickle as pkl\n",
        "import os\n",
        "years=[\"2012\", \"2013\", \"2014train\", \"2014test\", \"2016\", \"2019\"]\n",
        "#years = [\"2019\"]\n",
        "#train=False\n",
        "#name=\"LG\"\n",
        "error_files=[]\n",
        "bezier_curve_errors=[]\n",
        "parse_traces_errors=[]\n",
        "pop_errors=[]\n",
        "bad_relation_files=[]\n",
        "missing_relations=set()\n",
        "missing_relations_freq=defaultdict(lambda: 0)\n",
        "failures=0\n",
        "#datapath=\"C:/Users/aiden/Downloads/ProjectData/\"+year+\"/\"+ (\"Train/\" if train else \"Test/\") + name\n",
        "X=[]\n",
        "Y=[]\n",
        "count=0\n",
        "with open(\"FeatureData/Vocab.pkl\", \"rb\") as f:\n",
        "    vocab = pkl.load(f)\n",
        "for year in years:\n",
        "    directory=\"FeatureData/INKMLs/\"+year+\"/\"\n",
        "    for filename in os.listdir(directory):\n",
        "        #the filename unincluding .inkml\n",
        "        lgpath=convert_inkml_to_lg(directory+filename)\n",
        "        #lgpath=\"C:/Users/aiden/Downloads/ProjectData/LGINKML/LGs/\"+year+\"/\"+filename[:len(filename)-6]+\".lg\"\n",
        "        if os.path.exists(lgpath):\n",
        "            LG, order = convert_relations(lgpath)\n",
        "            if order and LG:\n",
        "                X.append(parse_traces(directory+filename, order, tolerance=20))\n",
        "                Y.append(LG)\n",
        "            else:\n",
        "                if LG==0:\n",
        "                    bad_relation_files.append(lgpath)\n",
        "        else:\n",
        "            print(\"missing file:\")\n",
        "            print(lgpath)\n",
        "            print(filename)\n",
        "        count += 1\n",
        "\n",
        "        if count % 100 == 0: print(count)\n",
        "datapath=\"FeatureData/\"\n",
        "with open(datapath+\"X_Y_2\", \"wb\") as f:\n",
        "    pkl.dump((X,Y), f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_dtW52_LixJ"
      },
      "outputs": [],
      "source": [
        "#index_to_symbol = {}\n",
        "#for key in vocab.keys():\n",
        "#    index_to_symbol[vocab[key]] = key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OgnkWydLixJ"
      },
      "outputs": [],
      "source": [
        "getSvg(\"FeatureData/INKMLs/2014train/formulaire020-equation005.inkml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDhKCf2aLixK"
      },
      "outputs": [],
      "source": [
        "#print(missing_relations)\n",
        "print(dict(sorted(missing_relations_freq.items(), key=lambda item: item[1])))\n",
        "#print(sorted(missing_relations_freq, key = lambda rel: missing_relations_freq[rel]))\n",
        "'''print(\"pop_errors:\")\n",
        "print(pop_errors)\n",
        "print(\"error_files\")\n",
        "print(error_files)\n",
        "print(\"bezier_curve_errors\")\n",
        "print(bezier_curve_errors)\n",
        "print(\"bad_relation_files:\")\n",
        "print(bad_relation_files)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRhkOTrHLixK"
      },
      "outputs": [],
      "source": [
        "Y[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7RFRyphLixL"
      },
      "outputs": [],
      "source": [
        "print(len(error_files))\n",
        "print(len(pop_errors))\n",
        "print(len(bad_relation_files))\n",
        "print(bezier_curve_errors)\n",
        "print(sum([missing_relations_freq[rel] for rel in missing_relations_freq.keys()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67riicVyLixL"
      },
      "outputs": [],
      "source": [
        "#testing parse_traces\n",
        "file=\"C:/Users/aiden/Downloads/ProjectData/LGINKML/INKMLs/2014train/109_leissi.inkml\"\n",
        "#print(convert_relations(file))\n",
        "#convert_relations(file)\n",
        "parse_traces(file, convert_relations(convert_inkml_to_lg(file))[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcdZb1biLixL"
      },
      "outputs": [],
      "source": [
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_4i503NLixM"
      },
      "outputs": [],
      "source": [
        "get_latex(\"C:/Users/aiden/Downloads/ProjectData/LGINKML/LGs/2014train/109_leissi.lg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnOhLjZrLixM"
      },
      "outputs": [],
      "source": [
        "with open(\"FeatureData/Vocab.pkl\", \"rb\") as f:\n",
        "    vocab=pkl.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "id": "a0XG1PsBS3bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPHHPjq3LixM"
      },
      "outputs": [],
      "source": [
        "datapath=\"FeatureData/\"\n",
        "with open(datapath+\"X_Y\", \"rb\") as f:\n",
        "    X, Y = pkl.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Abq_YVhRLixN"
      },
      "outputs": [],
      "source": [
        "length = len(X)\n",
        "vocab_size = len(vocab)\n",
        "X_train = []\n",
        "Y_train = []\n",
        "X_test = []\n",
        "Y_test = []\n",
        "X_eval = []\n",
        "Y_eval = []\n",
        "for i in range(length):\n",
        "    if (i%10>1):\n",
        "        X_train.append(X[i])\n",
        "        Y_train.append(Y[i])\n",
        "    elif (i%10):\n",
        "        X_eval.append(X[i])\n",
        "        Y_eval.append(Y[i])\n",
        "    else:\n",
        "        X_test.append(X[i])\n",
        "        Y_test.append(Y[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UppO7gzsLixN"
      },
      "outputs": [],
      "source": [
        "getSvg(\"C:/Users/aiden/Downloads/ProjectData/LGINKML/INKMLs/2012/001-equation000.inkml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI8H5DMeLixO"
      },
      "outputs": [],
      "source": [
        "datapath=\"FeatureData/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88zQKCO8LixO"
      },
      "outputs": [],
      "source": [
        "with open(datapath+\"Train\", \"wb\") as f:\n",
        "    pkl.dump((X_train, Y_train), f)\n",
        "with open(datapath+\"Test\", \"wb\") as f:\n",
        "    pkl.dump((X_test, Y_test), f)\n",
        "with open(datapath+\"Eval\", \"wb\") as f:\n",
        "    pkl.dump((X_eval, Y_eval), f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdXucTyrLixP"
      },
      "outputs": [],
      "source": [
        "with open(datapath+\"Train\", \"rb\") as f:\n",
        "    X_train, Y_train = pkl.load(f)\n",
        "with open(datapath+\"Test\", \"rb\") as f:\n",
        "    X_test, Y_test = pkl.load(f)\n",
        "with open(datapath+\"Eval\", \"rb\") as f:\n",
        "    X_eval, Y_eval = pkl.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mdgTLVCLixQ"
      },
      "outputs": [],
      "source": [
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHR4YcajLixR"
      },
      "outputs": [],
      "source": [
        "NUM_FEATURES=7\n",
        "len_train=len(X_train)\n",
        "len_test=len(X_test)\n",
        "max_X_train_len = max([len(sample) for sample in X_train])\n",
        "max_Y_train_len = max([len(sample) for sample in Y_train])\n",
        "max_X_test_len = max([len(sample) for sample in X_test])\n",
        "max_Y_test_len = max([len(sample) for sample in Y_test])\n",
        "padding_vector=np.ones(NUM_FEATURES)\n",
        "eos_index=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89nS6eqfLixR"
      },
      "outputs": [],
      "source": [
        "X_train_data_len=[len(data) for data in X_train]\n",
        "Y_train_data_len=[len(data) for data in Y_train]\n",
        "X_test_data_len=[len(data) for data in X_test]\n",
        "Y_test_data_len=[len(data) for data in Y_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BvJh5jhLixR"
      },
      "outputs": [],
      "source": [
        "padded_X_train=np.zeros((len_train, max_X_train_len, NUM_FEATURES))+padding_vector\n",
        "padded_Y_train=np.zeros((len_train, max_Y_train_len))+eos_index\n",
        "padded_X_test=np.zeros((len_test, max_X_test_len, NUM_FEATURES))+padding_vector\n",
        "padded_Y_test=np.zeros((len_test, max_Y_test_len))+eos_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sfn7taXZLixS"
      },
      "outputs": [],
      "source": [
        "for i in range(len_train):\n",
        "    sample_X = np.array(X_train[i])\n",
        "    padded_X_train[i,:len(sample_X),:]=sample_X\n",
        "    sample_Y = np.array(Y_train[i])\n",
        "    padded_Y_train[i,:len(sample_Y)]=sample_Y\n",
        "        \n",
        "for i in range(len_test):\n",
        "    sample_X = np.array(X_test[i])\n",
        "    padded_X_test[i,:len(sample_X),:]=sample_X\n",
        "    sample_Y = np.array(Y_test[i])\n",
        "    padded_Y_test[i,:len(sample_Y)]=sample_Y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiw8OFfgLixS"
      },
      "outputs": [],
      "source": [
        "getSvg(inkml_file_path+\"2012/001-equation000.inkml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25ab3r1YLixS"
      },
      "outputs": [],
      "source": [
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "id": "g8xTuDDggfBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozBnZZD0LixS"
      },
      "outputs": [],
      "source": [
        "Y[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CD6-IFYJLixT"
      },
      "outputs": [],
      "source": [
        "print(len(Y_train))\n",
        "print(len(X_test))\n",
        "print(len(X_eval))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "6737d4ed9008f34a36365f9afa168c058c7bcfc27ae9ebfad5913b219a0345d6"
      }
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}